{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Miniplaces1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAgQTTYNpA9x"
      },
      "source": [
        "# 6.869 Miniplaces Challenge - Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErA4keUJB2TQ"
      },
      "source": [
        "The miniplaces challenge is a 2 part challenge. Each part counts for 1 pset. \n",
        "In the challenge, you will work on classifying scenes into one of several categories (such as \"desert\", or \"forest\")\n",
        "\n",
        "In this part, we'll use pretrained weights on a different dataset, but one that's also used for scene classification. We'll examine how we can visualize feature maps, to better understand how a neural net came to a decision about a particular scene.\n",
        "\n",
        "Next week, you'll implement your own neural net to do scene classification, and try to improve it as much as you can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_wzeAuZpIhG"
      },
      "source": [
        "# Requirements installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-mfzOA1NQvl"
      },
      "source": [
        "First, let's install everything needed to run this notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GdEXtjp-QX_"
      },
      "source": [
        "!pip install Pillow\n",
        "!pip install -U image\n",
        "!pip install opencv-python\n",
        "\n",
        "from io import BytesIO\n",
        "from IPython.display import clear_output, Image, display\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAP97sYrNa-W"
      },
      "source": [
        "We will load PyTorch, our main tool to play with neural networks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh9nAQTrIx7V"
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "import torch.hub\n",
        "\n",
        "from os.path import exists\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHEfVC-to-5V"
      },
      "source": [
        "# Loading Images and PyTorch models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwXeYaYCNisx"
      },
      "source": [
        "Once, we have loaded all the relevant libraries, we will load the model. We will begin with an scene classification model trained on the Places Dataset with a ResNet-50 architecture.\n",
        "\n",
        "![texto alternativo](https://www.codeproject.com/KB/AI/1248963/resnet.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxETSgjcC7tb"
      },
      "source": [
        "# Helper function to download things without wget\n",
        "import requests\n",
        "def download(url, fn=None):\n",
        "  if fn is None:\n",
        "    fn = url.split('/')[-1]\n",
        "  r = requests.get(url)\n",
        "  if r.status_code == 200:\n",
        "      open(fn, 'wb').write(r.content)\n",
        "      print(\"{} downloaded: {:.2f} KB\".format(fn, len(r.content)/1024.))\n",
        "  else:\n",
        "      print(\"url not found:\", url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jz1hAVlpN-3y"
      },
      "source": [
        "We will load the pretrained weights into the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wabOVv7q-n9e"
      },
      "source": [
        "# Initialize the model\n",
        "resnet = models.resnet50(num_classes=365)\n",
        "# Download the pretrained weights\n",
        "download('http://places2.csail.mit.edu/models_places365/resnet50_places365.pth.tar')\n",
        "# sd = torch.load('resnet50_places365.pth.tar') ['state_dict'] ## Uncomment this line if you have a GPU or are using Colab\n",
        "sd = torch.load('resnet50_places365.pth.tar', map_location=torch.device('cpu')) ['state_dict'] ## Uncomment this line if you don't have a GPU\n",
        "\n",
        "# When a model is trained using parallelism, the weights begin with \"module.\"\n",
        "# Since we aren't going to be using parallelism, we'll manually change these keys to load the state dict\n",
        "sd = {k.replace('module.', ''): v for k, v in sd.items()}\n",
        "resnet.load_state_dict(sd)\n",
        "resnet.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bF8-ICwh6z7"
      },
      "source": [
        "# Visualizing Network Filters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6WiSqkTX_bd"
      },
      "source": [
        "First, we will define a function to display images from numpy arrays. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqH3GvHyoq-s"
      },
      "source": [
        "def showarray(a, fmt='jpeg'):\n",
        "    a = np.uint8(np.clip(a, 0, 255))\n",
        "    f = BytesIO()\n",
        "    PIL.Image.fromarray(a).save(f, fmt)\n",
        "    display(Image(data=f.getvalue()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3NL-WaTYN1l"
      },
      "source": [
        "Now, let's write a function to visualize the filters. You have to complete the following code, with one line normalizing the filter values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S34CUWPkgLW"
      },
      "source": [
        "def visualize_filters(conv_w, output_size = None):\n",
        "    w_normalized = None #TODO: Normalize conv_w values to 0-1 range  \n",
        "    map_t = 255*w_normalized\n",
        "    map_t = map_t.numpy()\n",
        "    map_t = map_t.astype(np.uint8)\n",
        "    if output_size is not None:\n",
        "        map_t = cv2.resize(map_t,(output_size,output_size))\n",
        "    return map_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9tBWPAKYjo4"
      },
      "source": [
        "We will display the filters of the initial convolutional layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5cWPa_4Ye89"
      },
      "source": [
        "print(resnet.conv1.weight.data.size()) # Print the size of conv1 weights\n",
        "for i in range(30):\n",
        "  print('Visualizing conv1 filter', i)\n",
        "  vis = visualize_filters(resnet.conv1.weight.data[i,0,:,:],50)\n",
        "  showarray(vis)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iQxKhYP6NLe"
      },
      "source": [
        "## Exercise: Visualize filters for another convolutional layer in ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqZbldo0YjAD"
      },
      "source": [
        "#TODO: pick a different layer to analyze\n",
        "for i in range(30):\n",
        "  print('Visualizing filter', i)\n",
        "  vis_conv = visualize_filters(None, 50)\n",
        "  showarray(vis_conv)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hykO_Di6iDI6"
      },
      "source": [
        "# Predicting classes with a pre-trained model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkNZ-26zOQfO"
      },
      "source": [
        "To make the process easier to read, we will load the label <--> index assignament for the Places dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Xx19KEH2aV"
      },
      "source": [
        "from urllib.request import urlopen\n",
        "synset_url = 'http://gandissect.csail.mit.edu/models/categories_places365.txt'\n",
        "classlabels = [r.split(' ')[0][3:] for r in urlopen(synset_url).read().decode('utf-8').split('\\n')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqRIs5sFZa5c"
      },
      "source": [
        "We will load one image to use through the pset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKRBAIWRFQRq"
      },
      "source": [
        "from torchvision import transforms\n",
        "download('http://6.869.csail.mit.edu/fa19/miniplaces_part1/rio.jpg')\n",
        "img_pil = PIL.Image.open('rio.jpg').convert('RGB')\n",
        "img_numpy = np.array(img_pil)\n",
        "showarray(img_numpy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCwHmgo1pmpF"
      },
      "source": [
        "First, let's take a look at the raw prediction of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCHZcX0lpvzb"
      },
      "source": [
        "  #data preprocessing: resize an image, change it from a PIL image to a pytorch tensor, normalize it according to dataset statistics\n",
        "  center_crop = transforms.Compose([\n",
        "         transforms.Resize((227,227)),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "  ])\n",
        "  \n",
        "  #our model can only take input that is preprocessed, so preprocess our loaded image\n",
        "  img_tensor = center_crop(img_pil)\n",
        "  logits = resnet(img_tensor.unsqueeze(0)).squeeze()\n",
        "  # get the indices associated with the topk logits\n",
        "  categories = logits.topk(5)[1] \n",
        "    \n",
        "  #print the labels corresponding to the topk indices\n",
        "  print(categories)\n",
        "  print(', '.join([classlabels[cat] for cat in categories]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znOait62TwB5"
      },
      "source": [
        "# Visualizing Internal Activations of the Network\n",
        "\n",
        "Let's look at what parts of the image cause different units to activate (send some positive signal). All of these activations combine to inform the final inference. \n",
        "\n",
        "The convolutional layers of ResNet essentially make a semantic representation of what is contained in the image. This is followed by two fully connected layers, which use the information from that representation to categorize the image.\n",
        "\n",
        "So, let's remove the last few layers (which do classification) to get the underlying representation, and we'll visualize the activations that went into that representation from different units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drMqLrhbb8_4"
      },
      "source": [
        "def generate_featuremap_unit(model_cut,unit_id,im_input):\n",
        "    # Extract activation from model\n",
        "    # Mark the model as being used for inference\n",
        "    model_cut.eval()\n",
        "    # Crop the image\n",
        "    im = center_crop(im_input)\n",
        "    # Place the image into a batch of size 1, and use the model to get an intermediate representation\n",
        "    activations = model_cut(im.unsqueeze(0))\n",
        "    # Print the shape of our representation\n",
        "    print(activations.size())\n",
        "    # Extract the only result from this batch, and take just the `unit_id`th channel\n",
        "    # Return this channel\n",
        "    return activations.squeeze()[unit_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1-brxSoT6VY"
      },
      "source": [
        "def visualize_featuremap(im_input,feature_map,alpha=0.3):\n",
        "    # Normalize to [0..1], with a little leeway (0.9999) in case feature_map has 0 range\n",
        "    feature_map = feature_map/(feature_map.max()+1e-10)\n",
        "    # Convert to numpy (detach() just seperates a tensor from the gradient)\n",
        "    feat_numpy = feature_map.detach().numpy()\n",
        "    # Resize the feature map to our original image size (our strided conv layers reduce the size of the image)\n",
        "    feat_numpy = cv2.resize(feat_numpy,(im_input.shape[1],im_input.shape[0]))\n",
        "    # Invert to make the heatmap look more natural\n",
        "    map_t = 1-feat_numpy\n",
        "    # Add an extra dimension to make this a [H,W,C=1] image \n",
        "    feat_numpy = np.expand_dims(feat_numpy, axis=2)\n",
        "    \n",
        "    # Convert to image (UINT8 from 0-255)\n",
        "    map_t = 255*map_t\n",
        "    map_t = map_t.astype(np.uint8)\n",
        "    # Use a color map to change this from BW to a nice color\n",
        "    map_t = cv2.applyColorMap(map_t, cv2.COLORMAP_JET)\n",
        "    # Combine the heatmap with the original image so you can see which section of the image is activated\n",
        "    im_final = np.multiply((alpha*im_input + (1-alpha)*map_t), feat_numpy) + np.multiply(im_input, 1-feat_numpy)\n",
        "    # Return final visualization\n",
        "    return im_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I3Gv0furojs"
      },
      "source": [
        "# TODO: remove the last 2 layers of resnet \n",
        "# Note: the .children() function of nn.Module (which resnet50 inherits from) and nn.Sequential() will be useful\n",
        "model_cut = None "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wth2uzqQiOcg"
      },
      "source": [
        "# TODO: search for mountain, sky, and building units\n",
        "feat = generate_featuremap_unit(model_cut, 300, img_pil)\n",
        "im_final = visualize_featuremap(img_numpy,feat)\n",
        "showarray(im_final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh6l1EaXT7JH"
      },
      "source": [
        "# (6.869 required) Find the unit index that has the maximum weights in the fully connected layer and deactivate that unit. Compare the orginal prediction and the new prediction\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "out_original = resnet(img_tensor.unsqueeze(0)).squeeze() #original prediction \n",
        "class_ids = np.argsort(-out_original.data.cpu().numpy())[0]\n",
        "index = torch.topk(resnet.fc.weight[class_ids,:], k=5)[1]\n",
        "\n",
        "# Get the output features for this model\n",
        "features = model_cut(img_tensor.unsqueeze(0))\n",
        "# Shape is now (1, # units, H, W)\n",
        "# TODO: deactivate the unit index that has the maximum weights (Set all values for that unit to 0)\n",
        "# TODO: run the modified features through the last two layers of the original network\n",
        "out_modified = None\n",
        "\n",
        "def plot_top_classes(values, top_k=5, title = None):\n",
        "  sorted_classes = np.argsort(-values)\n",
        "  class_ids = sorted_classes[:top_k]\n",
        "  class_names = [classlabels[it] for it in list(class_ids)]\n",
        "  class_values = values[class_ids]\n",
        "  print(title + \" top 5 class names \", class_names)\n",
        "  print(title + \" top 5 class values \", class_values)\n",
        "  plt.bar(class_names, class_values)\n",
        "  plt.xticks(rotation=60)\n",
        "  plt.title(title)\n",
        "\n",
        "plt.figure(0)\n",
        "plot_top_classes(out_original.data.cpu().numpy(), title = 'Original')\n",
        "plt.figure(1)\n",
        "plot_top_classes(out_modified.data.cpu().numpy(), title = 'Modified')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j8HRuNSqpmK"
      },
      "source": [
        "# Visualizing model activations with Class Activation Maps (CAMs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFgLrvzcqwzb"
      },
      "source": [
        "Once we have load the image and the model, now we will explore how to visualize the internal activations of the model. We will start by visualizing which parts of the image are responsibe for the final decision. \n",
        "\n",
        "![texto alternativo](https://camo.githubusercontent.com/fb9a2d0813e5d530f49fa074c378cf83959346f7/687474703a2f2f636e6e6c6f63616c697a6174696f6e2e637361696c2e6d69742e6564752f6672616d65776f726b2e6a7067)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6sDff8uOmQl"
      },
      "source": [
        "We create a version of the model without the last two layers, so that we can access the last convolutional layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wXXopizPDK-"
      },
      "source": [
        "We compute the activations using the Class Activation Mapping for a given output label. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "aTgSTx9QMhOB"
      },
      "source": [
        "def generate_featuremap_CAM(model,unit_id,im_input):\n",
        "    #Extract activation from model\n",
        "    model.eval()    \n",
        "    im = center_crop(im_input)\n",
        "    activations = model(im.unsqueeze(0)).squeeze() #2048 x h x w\n",
        "    num_channels, height, width = activations.shape\n",
        "    \n",
        "    # TODO: convert the shape of the output (out variable) to (h*w) x c \n",
        "    # The .view() function and .transpose() functions will help\n",
        "    activations_reshaped = None\n",
        "\n",
        "    # TODO: Run the fully connected layer from resnet to compute the weighted average with activations as the input variable\n",
        "    # out_final should be a h x w x 365 tensor. \n",
        "    out_final = None\n",
        "    \n",
        "    # TODO: obtain the class activation map for the corresponding unit_id\n",
        "    # class_activation_maps should be a 365 x height x width tensor. \n",
        "    class_activation_maps = None\n",
        "    return class_activation_maps[unit_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKz611GGPP9x"
      },
      "source": [
        "We can visualize the most activated region in the image for the 5 main top classes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rksAUc1fDR-1"
      },
      "source": [
        "for i in range(categories.shape[0]):\n",
        "  print('Visualizing category',classlabels[categories[i]])\n",
        "  feat = generate_featuremap_CAM(model_cut, categories[i].item(), img_pil)\n",
        "  im_result = visualize_featuremap(img_numpy,feat)\n",
        "  showarray(im_result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCNrg_0croju"
      },
      "source": [
        "#Optional: Try with your own image. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ri-xjKurojv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}